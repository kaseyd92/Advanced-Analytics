---
title: "Class 2"
output:
  html_document:
    df_print: paged
---

```{r}
# Clear workspace 
# This will clear your Global Environment of all files to start fresh. If you do not want to do this, change {r} to {r, eval = FALSE} for this code chunk.
rm(list = ls())
options(stringsAsFactors = FALSE)
```

```{r, set.seed(1234)}
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Overview of What we are going to today in this code
* Do a total Exploratory Data Analysis (EDA) walk through of HR date
* Run some chi-square and t-tests to look for differences in data slices
* Run a linear regression
* Introduce Tidymodels
  - parsnip - provide different statistical models
  - workflows - bundle your pre-processing, modeling, and post-processing together
* Run a logistic regression


From: (https://draganapavlovich.com/2018/09/22/human-resources-analytics-in-r/)

```{r, echo = FALSE, include= FALSE, warning = FALSE, message = FALSE}
#' <!-- ####################################################################################################### -->
#' <!-- ####################################################################################################### -->
#' <!-- ##################################LOADING PACKAGES##################################################### -->

tryCatch(require(pacman),finally=utils:::install.packages(pkgs='pacman',repos='http://cran.r-project.org'));
require(pacman)

#' <!-- ##if the above doesn't work, use this code## -->
#' <!-- ##tryCatch -->
#' <!-- #detach("package:pacman", unload = TRUE) -->
#' <!-- #install.packages("pacman", dependencies = TRUE) -->
#' <!-- # ## install.packages("pacman") -->

pacman::p_load(readxl,
               readr,
               dplyr,
               tidyr,
               ggplot2,
               knitr,
               curl,
               DT,
               modelr,
               broom,
               purrr,
               pROC,
               rsample,
               conflicted,
               tidymodels,
               tidyposterior,
               easystats,
               report,
               tidylog
               
)

#' <!-- #Loading from GitHub -->
#' <!-- #pacman::p_load_current_gh("trinker/lexicon", "trinker/sentimentr") -->
```

```{r, echo = FALSE, include= FALSE, warning = FALSE, message = FALSE}

#' <!-- #Loading libraries -->

suppressPackageStartupMessages({
    library(readxl) # reading in Excel files
    library(readr) # reading in CSV files
    library(dplyr) # data manipulation
    library(tidyr) # Tidy Messy Data and pivot_longer and pivot_wider
    library(broom) # takes the messy output of built-in functions in R, such as lm, nls, or t.test, and turns them into tidy tibbles
    library(ggplot2) # data visualization
    library(knitr) # knitting data into HTML, Word, or PDF)
    library(curl) # a modern and flexible web client for R
    library(DT) # provides an R interface to the JavaScript library DataTables
    library(modelr) # Functions for modelling that help you seamlessly integrate modelling into a pipeline of data manipulation and visualisation
    library(purrr) # enhances R’s functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors
    library(pROC) # display and analyze ROC curves
    library(rsample) # provides functions to create different types of resamples and corresponding classes for their analysis
    library(conflicted) # provides and alternate way of resolving conflicts in R
    library(tidymodels) # framework for modeling and machine learning using tidyverse principles
    library(rsample) # provides infrastructure for efficient data splitting and resampling
    library(parsnip) # creates a universal interface for various ML packages in R to use within tidymodels
    library(recipes) # a tidy interface to data pre-processing tools for feature engineering
    library(workflows) # bundle your pre-processing, modeling, and post-processing together
    library(tune) # helps you optimize the hyperparameters of your model and pre-processing steps
    library(yardstick) # measures the effectiveness of models using performance metrics
    library(dials) # creates and manages tuning parameters and parameter grids
    library(tidyposterior) # conduct post hoc analyses of resampling results generated by models
    library(easystats) # Create nice reports of statistical output including effect size
    library(report) # Create nice reports of statistical output including effect size
    library(tidylog) # Creates a log to tell you what your tidyverse commands are doing to the data. NOTE: MAKE SURE TO ALWAYS LOAD LAST!!!
})

# From: https://rdrr.io/github/Bio302-UiB/data-handling/f/inst/tutorials/using-dplyr/using-dplyr.Rmd
# use purrr::map to iterate over tidylog functions to prevent conflict with dplyr
# map(getNamespaceExports("tidylog"), ~conflict_prefer(.x, "tidylog", quiet = TRUE))

for (f in getNamespaceExports("tidylog")) {
    conflicted::conflict_prefer(f, "tidylog", quiet = TRUE)
}

# Tidylog run through
# From: https://github.com/elbersb/tidylog/blob/master/README.Rmd


```

```{r}
# Setting conflict_prefer
conflict_prefer("rmse", "yardstick")
conflict_prefer("chisq.test", "stats")
```


# Human Resources Analytics with R

[Link](https://draganapavlovich.com/2018/09/22/human-resources-analytics-in-r/)

Human Resources Analytics, also known as people, workforce or talent analytics is data-driven approach to managing people at work. Some of the application of HR analytics are – reducing accidents in a workplace, understanding and improving employee fairness and diversity and identifying drivers of employee attrition. Even so, using data to analyze and optimize the people aspect of business operations is only recently beginning to become widespread.

Real employee data cannot usually be shared outside a company due to privacy and ethical concerns. The dataset that we’ll be using are synthetic ones, produced by IBM. We will perform several analyses to answer questions about a company’s workforce. Most of the analyses we will do in HR analytics can be tackled in three general steps:

* identifying the groups to compare
* calculating summary statistics about those groups
* plotting or testing the differences between those groups

To perform HR analytics, we will use packages from tidyverse group, mainly dplyr, ggplot2 and tidyr. Full code and datasets can be found on this [link](https://github.com/Dragana236/Data_analysis).

### Importing recruitment data

```{r}
#From: https://stackoverflow.com/questions/14441729/read-a-csv-from-github-into-r
library(readr)
recruitment <- read_csv("https://raw.githubusercontent.com/Dragana236/Data_analysis/master/recruitment_data.csv")
```


## Identifying the Best Recruiting Source
It is very important to know where our best hires coming from. In this part we will try to discover which recruiting channel produces the best hires. However, before even start we need to get acquainted with our data. Our data frame is stored within ‘recruitment’ variable and first ten observations are shown below.

In every analysis it is recommended to explore all variables. We will start exploring with `summary()` function. Output of the function is given below and it seems like the distributions for numeric variables are normal and there are no outliers.

```{r}
summary(recruitment)
```

Function `summary()` didn’t tell us much about the ‘recruiting_source’ variable, because `read_csv()` imported it as a column of class character. Therefore, we will use `count(`) on the ‘recruiting_source’ column to get more information. Notice that base R function `table()` will produce similar counts as well but in form of classic data frame (not tibble). We see that most employees were employed through online platform besides these that don’t have a recruiting source.

```{r}
table(recruitment$recruiting_source)
```
We could also use the `tabyl` function from `janitor` which I like a little better than `table` since it gives you percent and valid percent.

```{r, warning = FALSE}
library(janitor)
tabyl(recruitment$recruiting_source)
```

Can we clean that up a bit?

Yes! Start [here](https://garthtarr.github.io/meatR/janitor.html) with a nice vignette and get to know the `adorn` functions

```{r}
recruitment %>%
  tabyl(recruiting_source) %>% # instead of using the $, I just piped it
  adorn_pct_formatting(digits = 0, affix_sign = TRUE)
```


We saw how many applicants were hired from each recruitment source, but remember our question was what is the best recruitment source. The question that arises is how do we measure quality of hire?  For this purpose, we can use metrics such as retention (how long the employee stays), hiring manager satisfaction, job performance or ‘time-to-productivity’ (the amount of time needed for a employee to become fully productive). All these metrics are in our dataset.

```{r}
names(recruitment)
```

Quality of hire metric that we will first use is ‘sales_quota_pct’. This is actually a measure of job performance and is referred to how much a sales person sold last year relative to their quota (sales quota attainment). For example, an employee whose ‘sales_quota_pct’ equals .75 sold 75% of their quota. This metric can be helpful because raw sales numbers are not always comparable between employees. Therefore, let’s calculate the average sales quota attainment achieved by hires from each recruiting source and see what department has the highest sales quota attainment. If we don’t take into account NA values, the source that gives hires with best average sales quota attainment is online applying for a job.

```{r}
avg_sales <- recruitment %>%
  group_by(recruiting_source) %>% 
  summarise(avg_sales_quota_pct = round(mean(sales_quota_pct),2)) %>%
  arrange(desc(avg_sales_quota_pct))

avg_sales
```

Although we can draw conclusion from our small tibble, it is always best practice to show differences visually (especially in cases when the number or observations is big). A bar chart with average sales per recruiting sources is shown below.

```{r}
ggplot(avg_sales, aes(x = recruiting_source, y = avg_sales_quota_pct)) +
  geom_col()
```

Let’s now consider another quality of hire metric – attrition, or how often hires leave the company. We want to determine which recruiting channels have the highest and lowest attrition rates. This measure can be calculated in a few different ways, but we will just divide number of employees that are not anymore in a company (denoted by 1 in ‘attrition’ column) by total headcount.

Attrition rates for each recruiting source are shown below. We see that highest attrition rate is in the case when company is looking for hires.

```{r attrition}
avg_attrition <- recruitment %>%
  group_by(recruiting_source) %>% 
  summarise(attrition_rate = round(mean(attrition),2)) %>% 
  arrange(desc(attrition_rate))

avg_attrition
```

Let’s also look at graphical representation of attrition rates. We plotted a bar chart again, but this time we used average attrition instead of sales quota attainment.

```{r}
ggplot(avg_attrition, aes(x = recruiting_source, y = attrition_rate)) +
  geom_col()
```

Finally let’s consider the last hire metric – ‘performance_rating’. We are interested in what hiring source produces hires with best performance ratings. From our output we can see that best sources are ‘Applied Online’ and ‘Campus’. Also, notice that mean value of performance rating is pretty low – only 2.93 out of 5.

```{r performance}
avg_performance <- recruitment %>%
  group_by(recruiting_source) %>% 
  summarise(highest_performance = round(mean(performance_rating),2)) %>% 
  arrange(desc(highest_performance))

avg_performance
```

From these explorations we can conclude that online source produces the best hires measured by attrition, sales and performance ratings. Also notice that we didn’t test if the differences are statistically significant. We will focus more on that later in the post.

## What is Driving Low Employee Engagement?
Now, we’ll look into potential reasons that one department’s engagement scores are lower than the rest. ‘Gallup’ – a performance-management consulting and research company defines engaged employees as those who are really into job, enthusiastic about and committed to their work and workplace. The idea is that employees who are more engaged are also more productive and stay with the organization longer. There is some evidence to support this relationship, but since there isn’t a single accepted definition of employees engagement, the connection between employees engagement and business outcomes is still debated. Engagement is usually measured with a survey, but behavioral data can be used as well. In this part, we will work with another dataset – ‘survey_data.csv’. Each employee has a single engagement score based on their responses in the survey. 5 is the most engaged and 1 is the least engaged.

### Importing survey data

```{r}
#From: https://stackoverflow.com/questions/14441729/read-a-csv-from-github-into-r
library(readr)
survey <- read_csv("https://raw.githubusercontent.com/Dragana236/Data_analysis/master/survey_data.csv")
```


Again, we will use `count()` function on the ‘department’ variable to check how many values each distinct level has. We see that the most employees are in engineering department. Take a close look at this variable, because the groups we are comparing are the different departments.

```{r exam}
survey %>% 
  count(department)
```

Let’s now see what department has the lowest engagement. We see that this department is ‘Sales’.

```{r}
survey %>%
  group_by(department) %>%
  summarise(avg_engagement = round(mean(engagement),2)) %>%
  arrange(desc(avg_engagement))
```

Next, we want to investigate and see what might be the cause of low engagement. For that purpose let’s create two groups – one with engaged and one with disengaged employees. The ones that have scores of 1 or 2 we will refer as disengaged, and ones with other scores as engaged. To create the ‘disengaged’ variable, we can use `mutate()` and `ifelse()` function. For more information on how `ifelse()` function works check out this [post](https://draganapavlovich.com/2018/09/07/categorical-variables/).

```{r engagement}
#Create disengaged variable
survey_disengaged <- survey %>% 
  mutate(disengaged = ifelse(engagement <= 2, 1, 0)) 

datatable(survey_disengaged, class = 'stripe row-border')

```

Let’s now calculate the percentage of employees that are disengaged, the average salary, and the average vacation days taken within each department. We see that ‘Sales’ department has the highest percentage of disengaged people and the lowest number of taken vacation days.

```{r}
#summarise disengaged, salary, and vacation days
survey_summary <- survey_disengaged %>% 
  group_by(department, disengaged) %>% 
  summarise(pct_disengaged = round(mean(disengaged),2),
            avg_salary = round(mean(salary),0),
            avg_vacation_days = round(mean(vacation_days_taken),2),
            n = n()) %>%
  ungroup()#remember to always pair your `group_by` with and `ungroup` statement!

survey_summary
```

Next, we can visually represent all these variables on one graphic. To get the most out of visualization, we have to use tidy dataset. Tidy form of our dataset would have two categorical and one numerical variable and graphic that is ideal for representing relationship between categorical variables is stacked or side-by-side bar chart.


```{r summary}
survey_summary <- survey_disengaged %>% 
  group_by(department) %>% 
  summarise(pct_disengaged = mean(disengaged),
            avg_salary = mean(salary),
            avg_vacation_days = mean(vacation_days_taken)) %>%
  ungroup()

survey_summary

survey_gathered <- survey_summary %>% 
  gather(key = "measure", value = "value",
         pct_disengaged, avg_salary, avg_vacation_days)
survey_gathered

ggplot(survey_gathered, aes(measure, value, fill = department)) +
  geom_col(position = "dodge")

```

Three side-by-side bar charts are shown below. Side-by-side bar charts are better option than stacked ones because we can easier compare the difference between departments by checking the heights of the bars. We put measure on the x-axis, value on the y-axis, and department as the fill aesthetic. What pops up is that last two bar charts are very tiny compared to first. Therefore we have to fix it.

Using faceting is a good option if we want different axes. In our case the y-axes extended to fit ‘avg_salary’, which was in the thousands. That meant that we couldn’t see the bars for the other two variables we were also trying to study. Faceting allows each variable plot to have its own y-axis.

By setting the ‘scales’ argument to ‘free’ we allowed the axes to vary by measure.

```{r}
ggplot(survey_gathered, aes(measure, value, fill = department)) +
  geom_col(position = "dodge") +
  facet_wrap(~ measure, scales = "free")
```

Graphics show what we saw above in the summary table – percentage of disengaged people is the highest in ‘Sales’ department as well as fewest average vacation days taken by this department. However, until we test it we can’t claim that ‘Sales’ department is significantly different from others.

## TESTING DIFFERENCES BETWEEN GROUPS

We’ve seen some evidence that the ‘Sales’ department has a higher proportion of disengaged employees than the rest of the company, but we aren’t yet certain if that difference is significant. To support that difference is statistically significant we have to use statistical test. To test differences between categorical variables we can use *chi-squared test*. This and p-test are used to determine how likely it is that the two samples are from different populations altogether. That likelihood is called the p-value. When the p-value is less than a pre-determined level such as 0.05, we can reasonably claim that these groups are not from same population, and that there is statistically significant difference. Besides t- and chi-squared tests, there are many statistical tests available, and the correct one to use depends on the data we are testing. If we had continuous data we would use t-test.

Since we are testing whether percentage of disengaged people is ‘Sales’ department is statistically different from other departments we should convert ‘department’ variable into categorical variable with two levels – ‘Sales’ and ‘Other’.

```{r convert categorial}
#Create categorical data for Sales and Other

survey_sales <- survey %>%
  mutate(in_sales = ifelse(department == "Sales", "Sales", "Other"))%>%
  mutate(disengaged = ifelse(engagement <= 2, 1, 0)) 

head(survey_sales)
```

Finally, we can use the chi-squared statistical test to test the hypothesis that the ‘Sales’ department has the same proportion of disengaged employees as the rest of the company.

```{r test}
conflict_prefer("chisq.test", "stats")

#Chi-square test
chisq.test(survey_sales$in_sales, survey_sales$disengaged) 
```

We decided on the `stats` version of `chisq.test` over `janior`'s version, but how are they different? Let's take a look.

```{r}
janitor::chisq.test(survey_sales$in_sales, survey_sales$disengaged)
```

It appears they are the same.

Since p-value is less than 0.05 level, we can say that difference between two groups is significant and two groups are not samples from the same population or in another words ‘Sales’ department has significantly higher percentage of disengaged employees than other departments.

What is the effect size on this?

Enter the `easystats` and `report` package!

Learn more about the `report` package [here](https://easystats.github.io/report/).

```{r}
library(easystats)
library(report)

chisq.test(survey_sales$in_sales, survey_sales$disengaged) %>% 
    report()
```

```{r}
chisq.test(survey_sales$in_sales, survey_sales$disengaged) %>% 
    report() %>%
    as.data.frame()
```

What is Cramer's V? Effect size calculation for Chi Square test of independence. Learn more [here](https://www.ibm.com/docs/en/cognos-analytics/12.0.0?topic=terms-cramrs-v).
What are the guidelines?

ES ≤ 0.2 - The result is weak. Although the result is statistically significant, the fields are only weakly associated.
0.2 < ES ≤ 0.6 - The result is moderate. The fields are moderately associated.
ES > 0.6 - The result is strong. The fields are strongly associated.


We also observed that employees in the ‘Sales’ department take fewer vacation days on average than the rest of the company. We can test whether that observation is statistically significant as well. Since now we have continuous variable we will have to use t-test.

As the main arguments the t-test takes a formula object and name of dataset. On one side of the ~ is the grouping variable, and on the other side is the variable we’re comparing. Notice the different syntax from chi-squared test.

```{r t-test}
t.test(vacation_days_taken ~ in_sales, data = survey_sales)
```

Looking at p-value, we can conclude that test result is significant at the 0.05 level, and that employees in ‘Sales’ department do take less vacation days than employees from other departments.

What is the effect size on the t-test? We know it is statistically significant, but how large is the effect?


```{r}
t.test(vacation_days_taken ~ in_sales, data = survey_sales) %>%
    report()
```

We are showing a medium effect size (d = 0.51) which is pretty good for the business world.

We can also output a data frame from `reports`.

```{r}
t.test(vacation_days_taken ~ in_sales, data = survey_sales) %>%
    report() %>%
    as.data.frame()
```



## Are New Hires Getting Paid Too Much?
When a company has a hard time filling a certain position, one option is to increase the offered salary to make the job more attractive. If the company does not increase the salary of current employees at the same rate that the market rate for new hires increases, the current employees will be paid less than the new hires. This is true even in jobs where the additional experience with the company should make the current employees more valuable. Further, this can cause employee turnover and dissatisfaction. Solution to this problem is doing routine checks of both, current employee and new hire salaries. Although this kind of analysis is usually done by compensation professionals, who are trained to understand nuances of market rates and job classifications we will also try to find out whether new hires are getting paid more than current employees. File that we want to analyze is ‘fair_pay.csv’. We are especially interested in three variables: ‘salary’, ‘new_hire’ and ‘job_level’.

### Importing pay data

```{r}
#From: https://stackoverflow.com/questions/14441729/read-a-csv-from-github-into-r
library(readr)
pay <- read_csv("https://raw.githubusercontent.com/Dragana236/Data_analysis/master/fair_pay.csv")
```


The first thing we need to do is to find the average salary of both, new hires and non-new hires. Output below shows that new hires are being paid more.


```{r inspecting}
pay %>% 
  group_by(new_hire) %>% 
  summarise(avg_salary = mean(salary))
```

We see that the new hires have a higher average salary than the more tenured employees, but we don’t yet know whether that difference is statistically significant. We can check this using the t-test. To obtain results of our statistical test in nicely-formatted data frame we use broom’s function `tidy()`.


```{r t-test2}
library(broom)
t.test(salary ~ new_hire, data = pay) %>%
  tidy()
```

Since the p-value is lower than 0.05, we can say that the result of the test is significant at the 0.05 level. There is a significant difference in salaries and this could be a problem.

```{r}
t.test(salary ~ new_hire, data = pay) %>%
    report()
```

Notice, the results are statistically significant, but the effect size is small.

Awesome!!!...so what would I say to a business executive about this?

"We conducted an analysis to compare the average salaries of new hires versus those who have been with the company for a longer time. On average, new hires earn about $2,650 more per year than their more tenured colleagues. While this difference is statistically significant, meaning it’s unlikely to have occurred by chance, the actual size of the difference is quite small.

In practical terms, this means that although there is a measurable difference in salaries between new hires and non-new hires, the impact on the overall salary structure is minimal. It’s important to note that while the difference exists, it is not large enough to have a major effect on our compensation strategy."

## OMITTED VARIABLE BIAS
We’ve seen how to test whether the difference between two groups is significant. A key assumption when comparing two groups is that the groups are the same except for the variable we are testing. Any conclusion we might want to draw from comparing two groups would by useless if there is some difference between the groups (say age when comparing two groups of people). When a missing variable is correlated with the dependent variable – the variable we’re comparing the groups on – and with the way the groups are divided, we are dealing with omitted variable bias. Missing variable is also known as a confounding variable. One variable we didn’t consider when investigated whether a new hires are paid more is job level variable. Different job levels tend to have different pay levels, so if the mix of job levels for new hires is different than the mix of job levels for current employees, we could be dealing with omitted variable bias.

One way to check whether two groups are similar in composition is to graph them. To check whether the job level composition is the same for new hires as it is for current employees, we will use a bar chart.

```{r plot}
pay %>% 
  ggplot(aes(x = new_hire, fill = job_level)) +
  geom_bar()
```

However, in these cases a good graph type to use is the 100% stacked bar chart. We see that the job level mix is different for new hires. New hires are less likely to be hourly employees, and more likely to be salaried or managers.

To answer if new hires have a higher average salary than current employees when job level is taken into account, we will first calculate the average salaries, and then recreate the earlier bar chart, adding faceting to split it up by the three job levels. We see that the bars are nearly equal. This supports the idea that an omitted variable – job level – is driving the difference in pay for new hires and current employees.

```{r in}
pay_grouped <- pay %>% 
  group_by(new_hire, job_level) %>% 
  summarise(avg_salary = mean(salary))

pay_grouped
```

```{r}
pay_grouped %>%
  ggplot(aes(x = new_hire, y = avg_salary)) +
  geom_col() +
  facet_wrap(~ job_level)
```


However, the graph shows a small difference in the average salaries for hourly employees. Just in case we will test whether a significant pay difference exists between hourly new hires and hourly current employees.

```{r ine}
pay_filter <- pay %>%
  filter(job_level == "Hourly")

t.test(salary ~ new_hire, data = pay_filter) %>%
  tidy()
```

Since the p.value is higher than 0.05, we can’t say that the result is significant at the 0.05 level. In another words the difference in salaries of hourly paid new hired and old hired employees is not statistically significant.

```{r}
t.test(salary ~ new_hire, data = pay_filter) %>%
    report()
```


### LINEAR REGRESSION

We’ve seen that the difference in salary between new hires and current employees disappears when another variable was taken into account. Looking at one variable alone is usually not enough to be confident that a meaningful difference exists. It would help if there was a way to test the significance of the difference between 2 groups while taking more other factors – such as omitted variables – into account.

Linear regression is another tool that can be used to test differences between groups, but that can also account for other factors. Linear regression can also be used in forecasting, optimization problems, and measuring the impact of one variable on another. For now,  we will focus on using regression to test the difference between groups. A linear regression will find an equation for the line that best fit each set of data. Conceptually, the best fit is the line that is closer to the points than any other line. In this case, the best fitting line will be the mean salary for each group.

We will start by using a simple linear regression to test the original idea that new hires earn more than current employees, without taking any other factors into account. To build a linear model, we will use the `lm()` function. The variable we’re comparing goes to the left of the tilde, and the grouping variable goes on the right.

```{r}
model_simple <- lm(salary ~ new_hire, data = pay)
```


To get more information about the model we use `summary()` on the resulting model.

```{r}
model_simple %>%
    summary()
```

To see more information, but now organized in a data frame we can again use tidy() function. Estimate in the first row is intercept and in the second row that is slope.

```{r}
model_simple %>%
    tidy()
```

According to regression coefficients we expect current employees to have an average salary of `$`73,424 and new hires to have an average salary `$`2,649 higher. Notice that this also matches the average salaries.

Since the p-value is less than 0.05, we can conclude that the test is significant at 0.05 level or in another words – new hire pay is significantly different according to this model. The answer on question which group has the higher average salary, we can get looking at estimate for ‘new_hireYes’. Since the estimate for ‘new_hireYes’ is positive, the result of this regression is that new hires have a significantly higher salary than current employees at the 0.05 level.

Can we use `report` on our linear regression?

```{r}
model_simple %>%
    report()
```

Yes we can!

Okay, how do we write this up?

Interpretation:
We fitted a linear regression model to predict salary based on whether an employee is a new hire or not. The results are:

R-squared (R²): 0.00387, indicating the model explains only about 0.387% of the variance in salary. This is a very weak relationship.
Adjusted R-squared: 0.00319, similar to R², adjusted for the number of predictors.
F-statistic: 5.70, with a p-value of 0.017, indicating the overall model is statistically significant.
Intercept: $73,424.60 (the average salary for non-new hires), with a 95% confidence interval of [$72,292.31, $74,556.90], highly significant (t = 127.20, p < .001).
Effect of new hire (Yes): The coefficient for new hires is $2,649.67, meaning new hires earn this amount more on average than non-new hires, with a 95% confidence interval of [$473.58, $4,825.77], statistically significant (t = 2.39, p = 0.017).
Standardized beta: 0.14, indicating a small effect size.

Explanation for a Business Executive:
"We conducted a statistical analysis to understand the impact of hiring status on salary. Our model shows that new hires earn about $2,650 more than long-term employees. This difference is statistically significant, meaning it's unlikely to be due to random chance.

However, the model explains less than 0.4% of the variation in salaries, indicating that hiring status alone does not account for much of the difference in salaries within our organization. Other factors are likely more important in determining salary levels.

In summary, while we see a small, significant increase in salaries for new hires, this factor alone doesn't explain much of the overall salary differences we observe. This insight can guide our compensation strategy, ensuring we look at a broader range of factors when setting salaries."

### ACCOUNTING FOR NEW VARIABLE
To get a significant result that takes the additional information into account, we can simply add the additional variable directly into the regression. In this case we want to take into account ‘job_level’ variable.

We’ll use multiple regression to test the difference in pay between new hires and current employees, while accounting for the fact that the job level mix is not the same in two groups. The syntax is almost identical to simple linear regression, except that we use the plus sign more, to add additional variables, in our case ‘job_level’.

```{r iner}
model_multiple <- lm(salary ~ new_hire + job_level, data = pay)
```

The output includes two more rows compared to one from simple regression. To see more information, we use `summary()` on our model. One advantage of using this output of model is that `summary()` function adds significant stars next to the p-values. If the p-value has at least one star next to it (which is not true in this case) the result is significant at 0.05 level.

```{r}
model_multiple %>%
  summary()
```
To see more information, but now organized in a data frame, we use `tidy()` function on our model.

```{r}
model_multiple %>%
  tidy()
```

Since we're testing the effect that being a new hire has on salary we will use p-value from new_hireYes row. Since it is higher than 0.05 we can say that new hires are not payed significantly higher in this model. 

New hires are being paid about the same as current employees when job level is considered.

```{r}
model_multiple %>%
    report()
```

Great! How would we write that up?


Interpretation:
We fitted a linear regression model to predict salary based on new hire status and job level. The results are:

R-squared (R²): 0.78, indicating the model explains 78% of the variance in salary, which is substantial.
Adjusted R-squared: 0.78, confirming the high explanatory power of the model.
F-statistic: 1711.74, with a p-value < .001, indicating the overall model is statistically significant.
Intercept: $64,049.34 (the average salary for non-new hires at the hourly job level), with a 95% confidence interval of [$63,444.50, $64,654.18], highly significant (t = 207.72, p < .001).
Within this model:

Effect of new hire (Yes): $782.74 higher on average than non-new hires, but not statistically significant (p = 0.136).
Effect of job level (Manager): $54,918.85 higher on average than hourly employees, highly significant (t = 60.00, p < .001), with a large standardized effect size (Std. beta = 2.90).
Effect of job level (Salaried): $26,865.63 higher on average than hourly employees, highly significant (t = 47.37, p < .001), with a large standardized effect size (Std. beta = 1.42).

Explanation for a Business Executive:
"We conducted an analysis to understand how new hire status and job level impact salaries. Our model reveals that these factors explain a significant portion of the variation in salaries within our organization.

Key findings include:

Job Level Impact: Managers earn about $54,919 more than hourly employees, and salaried employees earn about $26,866 more than hourly employees. These differences are highly significant and indicate that job level is a major determinant of salary.
New Hire Status: New hires earn about $783 more than non-new hires, but this difference is not statistically significant, suggesting that being a new hire does not have a meaningful impact on salary when job level is considered.
In summary, job level is the most important factor in determining salary differences, far outweighing the impact of new hire status. This insight can help us ensure that our compensation strategy is aligned with job responsibilities and levels, rather than just the hiring status of employees."

# Tidymodels

Last time I taught this class using bits and pieces of Machine Learning from various packages in R. Since then, much has come out around the `tidymodels` method of machine learning. Since `tidymodels` works well with the `tidyverse` and it was created by the same people who created the `caret` package (and wrote Applied Predictive Modeling), I will be switching over to `tidymodels` for the rest of the year as I think it weaves a more coherent message and ultimately allows you to test and deploy models in a much more streamline fashion.

Here are several good links to take you beyond what I'll show you for now.
* [Tidymodels for Machine Learning](https://hansjoerg.me/2020/02/09/tidymodels-for-machine-learning/)
* [Tidymodels: tidy machine learning in R](http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/)
* [A Gentle Introduction to tidymodels](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/)

And the ultimate walk through (seriously, its 8 parts with slides and code and everything!!!)
* [Introduction to Machine Learning with the Tidyverse](https://education.rstudio.com/blog/2020/02/conf20-intro-ml/)

And now there is a book by Max Kuhn and Julia Silge!
* [Tidy Modeling with R](https://www.tidymodels.org/books/tmwr/)

We will barrow elements from Introduction to Machine Learning with the Tidyverse next.

`Tidymodels` includes a core set of packages that are loaded on startup:

`broom` takes the messy output of built-in functions in R, such as lm, nls, or t.test, and turns them into tidy data frames.

`dials` has tools to create and manage values of tuning parameters.

`dplyr` contains a grammar for data manipulation.

`ggplot2` implements a grammar of graphics.

`infer` is a modern approach to statistical inference.

`parsnip` is a tidy, unified interface to creating models.

`purrr` is a functional programming toolkit.

`recipes` is a general data preprocessor with a modern interface. It can create model matrices that incorporate feature engineering, imputation, and other help tools.

`rsample` has infrastructure for resampling data so that models can be assessed and empirically validated.

`tibble` has a modern re-imagining of the data frame.

`tune` contains the functions to optimize model hyper-parameters.

`workflows` has methods to combine pre-processing steps and models into a single object.

`yardstick` contains tools for evaluating models (e.g. accuracy, RMSE, etc.)

There are a few modeling packages that are also installed along with tidymodels (but are not attached on startup):

`tidypredict` translates some model prediction equations to SQL for high-performance computing.

`tidyposterior` can be used to compare models using resampling and Bayesian analysis.

`tidytext` contains tidy tools for quantitative text analysis, including basic text summarization, sentiment analysis, and text modeling.


## Parsnip

To specify a model with `parsnip`
1. Pick a model
2. Set the engine
3. Set the mode (if needed)

To specify a model with `parsnip`
```{r, eval = FALSE}
decision_tree() %>% #pick a model
  set_engine("C5.0") %>% #set the engine
  set_mode("classification") #set the mode
```

```{r, eval = FALSE}
nearest_neighbor() %>% #pick a model
  set_engine("kknn") %>% #set the engine
  set_mode("regression") #set the mode
```

All available models are listed at

(https://www.tidymodels.org/find/parsnip/)

```{r, eval = FALSE}
linear_reg(
  mode = "regression", # "default" mode, if exists
  penalty = NULL, # model hyper-parameter # A non-negative number representing the total amount of regularization (glmnet, keras, and spark only). For keras models, this corresponds to purely L2 regularization (aka weight decay) while the other models can be a combination of L1 and L2 (depending on the value of mixture; see below).
  
  mixture = NULL # model hyper-parameter # A number between zero and one (inclusive) that is the proportion of L1 regularization (i.e. lasso) in the model. When mixture = 1, it is a pure lasso model while mixture = 0 indicates that ridge regression is being used. (glmnet and spark only).
)

#This will specify a model that uses linear regression (as we used above)
```

If you just asked, "what is a hyper-parameter" go [here](https://towardsdatascience.com/model-parameters-and-hyperparameters-in-machine-learning-what-is-the-difference-702d30970f6)

And a short definition from (towardsdatascience.com):
In a machine learning model, there are 2 types of parameters:
1. Model Parameters: These are the parameters in the model that must be determined using the training data set. These are the fitted parameters.
2. Hyper-parameters: These are adjustable parameters that must be tuned in order to obtain a model with optimal performance.

Now we will specify the model with `parsnip`.
```{r, eval = FALSE}
lm_spec %>%
  set_engine(engine = "lm", ...)
```

We can also set the mode which sets the class of problem the model will solve, which influences which output is collected. Not necessary if mode is set in Step 1.

```{r, eval = FALSE}
lm_spec %>%
  set_mode(mode = "regression")
```

### Your Turn 1

Now, write a pipe that creates a model that uses `lm()` to fit a linear regression. Save it as `lm_spec` and look at the object. What does it return?

Hint: you'll need (https://tidymodels.github.io/parsnip/articles/articles/Models.html)

```{r}
lm_spec <- 
  linear_reg() %>% # Pick linear regression
  set_engine(engine = "lm") # set engine

lm_spec
```
Here is the way to do this in base `tidymodels` using `fit`. 

```{r}
lm_fit <- fit(lm_spec,
              formula = salary ~ new_hire + job_level,
              data = pay)

lm_fit
```

Here is a way to do this using a custom function from [Alison Hill](https://education.rstudio.com/blog/2020/02/conf20-intro-ml/).
```{r}
# Custom formula 
# From: https://education.rstudio.com/blog/2020/02/conf20-intro-ml/

fit_data <- function(formula, model, data, ...) {
  wf <- workflows::add_model(workflows::add_formula(workflows::workflow(), formula), model)
  fit(wf, data, ...)
}

```


```{r}
lm_fit <- fit_data(salary ~ new_hire + job_level,
                   model = lm_spec,
                   data = pay)

lm_fit
```

## Your turn 2

Double check. Does:
```{r}
lm_fit <- fit_data(salary ~ new_hire + job_level,
                   model = lm_spec,
                   data = pay)

lm_fit
```

give the same results as:
```{r}
model_multiple <- lm(salary ~ new_hire + job_level, data = pay)

model_multiple
```

Yes! Notice that our `lm_fit` model is assigning a coefficent of 782.7 to new_hireYes and our original `model_multiple` is assigning a coefficient of 782.7 to new_hireYes as well.

Ok, now is where `tidymodels` really begins to shine.

`predict`-use a fitted model to predict new y values from data. Returns a tibble
```{r, eval = FALSE}
predict(lm_fit, new_data = pay)
```

For predictions, `parsnip` always conforms to the following rules:

1. The results are always a tibble.
2. The column names of the tibble are always predictable.
3. There are always as many rows in the tibble as there are in the input data set.

The row order of the predictions are always the same as the original data.

These three rules make it easier to merge predictions with the original data:

```{r}
pay %>%
  select(employee_id,
         salary) %>%
  bind_cols(predict(lm_fit, new_data = pay))
```
There is also nice feature that allows us to add 95% prediction intervals to the results:

```{r}
pay %>%
  select(employee_id,
         salary) %>%
  bind_cols(predict(lm_fit, new_data = pay)) %>% # This gives the predicted value
  bind_cols(predict(lm_fit, new_data = pay, type = "pred_int")) # This gives the 95% prediction intervals to the results
```

For more see (https://www.tmwr.org/models.html) Chapter 7.

We can also get some help from the `parsnip` addin in R Studio. You can call it using `parsnip_addin()` as follows:
```{r, eval = FALSE}
parsnip_addin()
```


## Your turn 3
Fill in the blanks. Use `predict()` to:
1. Use your linear model to predict pay; save tibble as pay_pred
2. Add a pipe (%>%) and use `mutate()` to add a column with the observed salaries; name it `truth`

```{r, eval = FALSE}
lm_fit <- fit_data(salary ~ new_hire + job_level,
                   model = lm_spec,
                   data = pay)

pay_pred <- ________ %>% 
  predict(new_data = ________) %>% 
  mutate(truth = ________)

pay_pred
```

## Your turn 3: Answer
```{r}
lm_fit <- fit_data(salary ~ new_hire + job_level,
                   model = lm_spec,
                   data = pay)

pay_pred <- lm_fit %>% 
  predict(new_data = pay) %>% 
  mutate(truth = pay$salary)

pay_pred
```

What is the goal of machine learning?

Generate accurate predictions!

What is the difference between the predicted and observed values called?

Residuals.

RMSE-Root Mean Squared Error - The standard deviation of the residuals about zero.

`rmse()`-Calculates the RMSE based on two columns in a dataframe:

*The truth: yi

The predicted estimate: y_hat_i

```{r, eval = FALSE}
rmse(data, truth, estimate)
```

```{r}
lm_fit <- fit_data(salary ~ new_hire + job_level,
                   model = lm_spec,
                   data = pay)

pay_pred <- lm_fit %>% 
  predict(new_data = pay) %>% 
  mutate(salary_truth = pay$salary) #noticed we changed this from `truth` to `salary_truth` for clarity

pay_pred
```

```{r}
conflict_prefer("rmse", "yardstick")

rmse(pay_pred, truth = salary_truth, estimate = .pred)
```



Axiom 1
The best way to measure a model's performance at predicting new data is to predict new data.

Goal of Machine Learning:
* construct models that 
* generate accurate predictions
* for future, yet-to-be-seen data

Max Kuhn & Kjell Johnston, (http://www.feat.engineering/)

Method 1-The holdout method (see slide)

`initial_split()`-"Splits" data randomly into a single testing and single training set.
```{r, eval = FALSE}
initial_split(data, prop = 3/4)
```

```{r}
pay_split <- initial_split(pay, prop = 0.75)
pay_split 
```

`training()` and `testing()`-Extract training and testing sets from an `rsplit`
```{r}
training(pay_split)
```
```{r}
testing(pay_split)
```



Lets assign them to an object
```{r}
train_set <- training(pay_split)
train_set
```

```{r}
test_set <- testing(pay_split)
test_set
```


### Quiz
Now that we have training and testing sets...

Which dataset do you think we use for fitting?

Which do we use for predicting?

## Your turn 4
Fill in the blanks.

Use `initial_split()`, `training()`, `testing()`, `lm()` and `rmse()` to:

1. Split `pay` into training and test sets. Save the rsplit!

2. Extract the training data. Fit a linear model to it. Save the model!

3. Measure the RMSE of your linear model with your test set.

Keep `set.seed(100)` at the start of your code.

```{r, eval = FALSE}
set.seed(100) # Important!
pay_split  <- ________
pay_train  <- ________
pay_test   <- ________
lm_fit      <- fit_data(salary ~ new_hire + job_level, 
                        model = lm_spec, 
                        data = ________)
pay_pred  <- ________ %>% 
  predict(new_data = ________) %>% 
  mutate(salary_truth = ________)
rmse(________, truth = ________, estimate = ________)
```

```{r}
set.seed(100) # Important!
pay_split  <- initial_split(pay)
pay_train  <- training(pay_split)
pay_test   <- testing(pay_split)

lm_fit      <- fit_data(salary ~ new_hire + job_level, 
                        model = lm_spec, 
                        data = pay_train)

pay_pred  <- lm_fit %>% 
  predict(new_data = pay_test) %>% 
  mutate(salary_truth = pay_test$salary)

rmse(pay_pred, truth = salary_truth, estimate = .pred)
```

RMSE = 9076.636	# Previous iteration (2023) was 8939.842. What is yours? Hopefully the set.seed(100) is working.

Normally, we would run our training set first, but we haven't done that since the split. Let's do that now to compare.

```{r}
pay_pred_train  <- lm_fit %>% 
  predict(new_data = pay_train) %>% 
  mutate(salary_truth = pay_train$salary)

rmse(pay_pred_train, truth = salary_truth, estimate = .pred)
```

RMSE = 9076.636 vs our test RMSE of 8874.587

### Quiz

How much data should you set aside for testing?

If testing set is small, performance metrics may be unreliable

If training set is small, model fit may be poor

## Cross-Validation

Ok, we have now split our data into training and test data.

What happens if the test data selected just happens to have data points that are really easy to predict?

What happens if the test data selected just happens to have data points that are really hard to predict?

Let's do some new splits and see what happens.

```{r}
all_wf <- 
  workflow() %>%
  add_formula(salary ~ new_hire + job_level) %>%
  add_model(lm_spec)
```

```{r}
new_split <- initial_split(pay)
```

The below code is not working. Not sure why.
```{r, eval = FALSE}
all_wf %>%
  fit_split(split = new_split,
            metrics = metric_set(rmse)) %>%
  collect_metrics()
```
Had to spell it out for the custom function below.

```{r, eval = FALSE}
fit_split(salary ~ new_hire + job_level, lm_spec, split = new_split, metrics = metric_set(rmse)) %>%
  collect_metrics()
```
Let’s say that we’ve concluded our model development and have settled on a final model. There is a convenience function called `last_fit()` that will fit the model to the entire training set and evaluate it with the testing set.

```{r}
final_lm_res <- last_fit(all_wf,
                         split = new_split,
                         metrics = metric_set(rmse) # Using the RMSE metric from the yardstick package for evaulation
                         )

final_lm_res
```
```{r}
fitted_lm_wflow <- extract_workflow(final_lm_res)

fitted_lm_wflow
```

Now, `collect_metrics()` and `collect_predictions()` provide access to the performance metrics and predictions, respectively.

```{r}
final_lm_res %>%
    collect_metrics()

```

```{r}
final_lm_res %>%
    collect_predictions()
```

Learn more about the `! Resample1: model (predictions): prediction from a rank-deficient fit may be misleading` [here](https://stackoverflow.com/questions/26558631/predict-lm-in-a-loop-warning-prediction-from-a-rank-deficient-fit-may-be-mis/26560328) and [here](https://stackoverflow.com/questions/40774922/how-to-solve-rank-deficient-fit-may-be-misleading-error-on-my-linear-model). I think we are OK in this instance, but I leave it up to you to investigate further.

## Cross-Validation

V-fold cross-validation
```{r, eval = FALSE}
vfold_cv(data, v = 10, ...)
```

## Quiz
If we use 10 folds, which percent of our data will end up in the training set and which percent in the testing set for each fold?

90% - training (10% + 10% + 10% + 10% + 10% + 10% + 10% + 10% + 10%)

10% - test

What if we did 5 folds?

80% - training (20% + 20% + 20% + 20%)

20% - test

## Your turn 6
Run the code below. What does it return?

```{r}
set.seed(100)
cv_folds <- vfold_cv(pay_train, v = 10)
cv_folds
```


`fit_resamples()`-Trains and tests a model with cross-validation

```{r, eval = FALSE}
#Fit with formula and model
fit_resamples(
  salary ~ new_hire + job_level, 
  model = lm_spec,          
  resamples = cv_folds
)
```

```{r}
#Fit with workflow
fit_resamples(
  all_wf, #remember when we put all of our info into all_wf? See below
  resamples = cv_folds
)

# all_wf <- 
#   workflow() %>%
#   add_formula(salary ~ new_hire + job_level) %>%
#   add_model(lm_spec)
```

`collect_metrics()`-Unnest the metrics column from a tidymodels `fit_resamples()`

```{r, eval = FALSE}
_results %>% collect_metrics(summarise = TRUE) #TRUE is the default; averages across folds
```

## Your turn 7

Modify the code below to use `fit_resamples()` and `cv_folds` to cross-validate the `all_wf` workflow.

Which RMSE do you collect at the end?


## Comparing Models

## Your turn 8

Create two new workflows, one that fits the `salary ~ new_hire + job_level` model and one that fits the whole model, `salary ~ .`

Then use `fit_resamples` and `cv_folds` to compare the performance of each.

```{r}
hire_job_wf <- 
  workflow() %>%
  add_formula(salary ~ new_hire + job_level) %>%
  add_model(lm_spec)

whole_wf <- 
  workflow() %>%
  add_formula(salary ~ .) %>%
  add_model(lm_spec)

```

```{r}
hire_job_wf %>%
  fit_resamples(resamples = cv_folds) %>%
  collect_metrics()
```

```{r}
whole_wf %>% 
  fit_resamples(resamples = cv_folds) %>%
  collect_metrics()
```

## Quiz
Why should you use the same data splits to compare each model?

Apples to Apples

## Quiz
Does Cross-Validation measure the accuracy of just your model, or your entire workflow?

Your entire workflow

Ok, now that we've done a `tidymodels` aside, let's get back to the main post [here](https://draganapavlovich.com/2018/09/22/human-resources-analytics-in-r/) and more specifically [here](https://github.com/Dragana236/Data_analysis/blob/master/Human_resource_analytics.Rmd) since the original post is not there anymore, where we were talking about multiple regression. We will briefly go back to an older method before delving into `tidymodels` again.

### Back to the main post

**Note:** Here we are going back to just the full `pay` data, but I would encourage you on your own to work through this with the `pay_split` data we just used above. You would not want to use the full `pay` data set as then you have nothing to test your model with. I'm only going back to the full `pay` data set as I want to show you the "how", but ultimately, I think a great deal of you learning will occur by you getting into this code on your own and trying the `pay_split` data with `vfold_cv`.

We can’t interpret the estimates exactly the same way as we did for simple linear regression, but we can still use p-value. To determine whether the test is significant at the 0.05 level, we will use p-value for ‘new_hireYes’, since we’re testing the effect that being a new hire has on salary. Since the p-value is greater than 0.05 we can’t say that new hire have significantly higher salary.

To test if the difference in salary is significant for new hire and current employees we used t-test and regressions. From these two tests we can conclude that new hires are being paid about the same as current employees when job level is taken into account.

However, let’s also take department into account instead of job_level. We can see that the p-value for the new hire is 0.017 which is less than 0.05, and the estimate for new_hireYes is positive. Therefore, the result of the multiple regression is that new hires do earn more than current employees even when department is taken into account.

```{r inerr}
model_multiple <- lm(salary ~ new_hire + department, data = pay) #notice this is also called "multiple model" even though it is now salary ~ new_hire + department instead of salary ~ new_hire + job_level
model_multiple%>%
  tidy()
```


While we're here, could we get the same output from using a `tidymodels` multiple regression model that we create called `hire_job_fit`?

Let's try!


```{r, eval = FALSE}
hire_job_fit <- fit_data(salary ~ ________ + ________, # What are our predictors (hint: look at the code chunk above)
                         model = _________, # What is our model? (hint: lm________)
                         data = ________) # What the data we are using?
```


```{r}
hire_job_fit <- fit_data(salary ~ new_hire + department,
                   model = lm_spec,
                   data = pay)
```

```{r}
hire_job_fit %>%
  tidy()
```

Yes!


### Are Performance Ratings Being Given Consistently?
To do analytics with any data, we frequently need to merge or join together data from different data sources. In this part of post we will use two datasets: ‘hr_data.csv’ and ‘performance_data.csv’. Merging these two data frames we will have performance rating data and some base HR data in one dataset. Once we have the final data frame, we will be able to see whether the average performance rating is higher for men or women.

### Importing HR and performance data and joining them

```{r}
#From: https://stackoverflow.com/questions/14441729/read-a-csv-from-github-into-r
library(readr)
hr_data <- read_csv("https://raw.githubusercontent.com/Dragana236/Data_analysis/master/hr_data.csv")

head(hr_data)


performance_data <- read_csv("https://raw.githubusercontent.com/Dragana236/Data_analysis/master/performance_data.csv")

head(performance_data)
```

### JOINING DATA

Now that we’ve looked at ‘hr_data’ and ‘performance_data’, it’s time to join them. To join datasets together we will use dplyr’s left_join() function. First two argument of the function are data frames that we want to join. The value of third argument ‘by’ is key that we want to join by.


```{r}
joined_data <- hr_data %>%
    left_join(performance_data, by = "employee_id")
```


In this case right table contains all keys from the left table. Therefore, all rows will match.

However, you may be wonder what happens when one table includes data of employees that are not found in another. The final data frame retains all values from left table and when there is no matching value in the right data frame it adds NA. If there was a key in the right data frame that don’t match to any in the left table, row with that key would be dropped. As for keys that are used for joins -the safest is to use employee id, or some other field that uniquely represents an employee. Using a number is safer than using the employee name because names are not always unique. Calling `head()` on our new data frame gives the following output.

```{r}
head(joined_data)
```

Most workplaces try to motivate employees to perform better by providing additional rewards to the highest-performing employees. To do that, the organization needs to evaluate the employees and sometimes compare them against one another. That seems straightforward, but most employees do not produce output that can be easily quantified, so the performance appraisal process is inherently subjective. Conscious bias is relatively rare. Most managers and leaders are not deliberately giving better performance ratings to one group over another in a biased or unfair way. On the other hand everybody is subject to some measure of unconscious bias. Here, we will be analyzing performance ratings and we will be checking whether women are significantly less likely to be labeled a high performer than men.

We will start by checking whether the average performance rating differs by gender. From output it looks like there may be some difference worth investigating.

```{r gender rate}
joined_data %>%
  group_by(gender) %>%
  summarise(avg_rating = round(mean(rating),2)) %>%
  ungroup()
```

Let’s now focus on high performers. Performance ratings matter to organizations and individual employees. Being labeled a ‘high performer’ can matter even more than the exact rating when the employee is considered for promotions, bonuses, and raises. Organizations can define ‘high performer’ however they wish, but we will define them as any employee with a rating of 4 or 5 on the 5-point scale. We will create new ‘high_performer’ variable, and perform a statistical test on whether the women in this dataset are significantly less likely to be labeled high performers.

```{r gender rate1}
performance <- joined_data %>%  
  mutate(high_performer = ifelse(rating >= 4, 1, 0))
head(performance)
```

After grouping and summarizing we can see that there is a difference between the proportion of high performers among females and males. However, to be sure that this difference is statistically significant we need to test it.

```{r}
performance %>%
  group_by(gender) %>%
  summarise(avg = round(mean(high_performer),3)) %>%
  ungroup()
```

We will test whether men are more likely to be high performers in this dataset, using the chi-squared test, since ‘high_performers’ is a categorical variable. Resulting p-value is less than 0.05, meaning that the test is significant at the 0.05 level, or in another words, there is statistically significant difference between males and females being high performers.


```{r test1}
chisq.test(performance$gender, performance$high_performer)%>% 
  tidy()
```

We just performed a statistical test on ‘high_performer’ variable. We can also visualize the difference with 100% stacked bar charts.

```{r vis9}
performance %>%
  ggplot(aes(gender, fill = factor(high_performer))) +
  geom_bar(position = "fill")  
```

But, let’s also create a more detailed bar chart using general ratings. The visualization matches what we found in the statistical test.

```{r vis1}
performance %>%
  ggplot(aes(gender, fill = factor(rating))) +
  geom_bar(position = "fill")
```

Let's look at it another way by swapping out `fill` for `dodge` to unstack the bars.

```{r}
performance %>%
  ggplot(aes(gender, fill = factor(rating))) +
  geom_bar(position = "dodge")
```


We also have to pay attention on omitted variable bias. In many organizations, employees at higher job levels in the organization are more likely to be considered high performers. That is, the distribution of performance ratings is not always the same at different job levels.

First, we want to check the difference in job level distribution by gender to see if there is any potential omitted variable that we need to consider. Again, we can see different mixture of distributions.

```{r vis5}
performance %>%
  ggplot(aes(x = gender, fill = job_level)) +
  geom_bar(position = "fill")
```

We also tested whether men and women have different job level distributions. Dependent variable, ‘job_level’ is categorical variable, we therefore used chi-squared test. Since the p-value is much lower than 0.05, we can conclude that there is significant difference in distribution of job level between genders.

```{r vis8}
chisq.test(performance$gender, performance$job_level) %>%
    report()
```



Finally, let’s visualize the distribution of ‘high_performer’ by gender, faceted by job level. We can see that even when differences in job levels are taken into account, women are less likely to be labeled high performers than men.

```{r vis2}
performance %>% 
  ggplot(aes(x = gender, fill = factor(high_performer))) +
  geom_bar(position = "fill") +
  facet_wrap(~ job_level)
```

For testing if some difference is significant, instead of using statistical tests we could have used regression. We saw that linear regression works well when the dependent variable is continuous such as salary. The power of the regression comes from its ability to fit a straight line to the data points with as little error as possible.

However, using a linear regression model is less effective when the dependent variable is binary variable – variable that can only have 2 values. We see that employees who are high performers are on the top line and employees that are not are on the lower line. A linear model will try to draw a straight line through the points the best it can. We can see that most of the endpoints are far from the blue line, which suggests that the model isn’t appropriate for this kind of data. The model is especially poor fit for high and low salaries.

#Image 1 here
![](Linear Regression.png)

For data with a binary dependent variable *logistic regression* is better suited than linear regression. We can see that the red line – the logistic fit – much closer fits to the data, even at the extreme salary levels.

#Image 2 here
![](Logistic Regression.png)


Our goal was to test whether men are statistically more likely to be labeled high performers in this dataset even when job level is taken into account. Since dependent variable is binary we need to use logistic regression. The syntax for logistic regression is a little bit different than syntax for linear regression. Now, we use the glm() function instead of lm(). glm stands for generalized linear model, and glm() can be used to fit a wide variety of models. To specify the type of regression model, we use the ‘family’ argument. For a logistic regression we use family = ‘binomial’.

Since the result of statistical test was significant – there is a difference in counts for high performers among gender, we won’t test it using logistic regression. We are more interested if women are less likely to be labeled a high performer, even when taking differences in job level into account. Since we now have two independent variables we will need to use, multiple logistic regression, which works much like multiple linear regression. We can take additional factors into account by appending them with the plus sign.

```{r vis3}
logistic_multiple <- glm(high_performer ~ gender + job_level, family = "binomial", data = performance)
logistic_multiple %>%
  tidy()
```

Can we get a `report` on this?
```{r}
logistic_multiple %>%
    report()
```



Ok, that is how we do logistic regression with base `R`. How about with `tidymodels`?

Create `log_spec`.

```{r, eval = FALSE}
log_spec <-
  _______ _reg() %>%
  set_engine(engine = "______") #Hint: what did we used instead of `lm` in the code chunk above?
```


```{r}
log_spec <-
  logistic_reg() %>%
  set_engine(engine = "glm")
```

```{r}
log_spec
```


```{r, eval = FALSE}
lr_fit <- fit_data(high_performer ~ gender + job_level,
                   model = log_spec,
                   data = performance)
```
Error: For a classification model, the outcome should be a factor.

```{r}
class(performance$high_performer)
```

Hmmm, how do we fix this?

```{r}
performance <- performance %>%
  mutate(high_performer = as.factor(high_performer))
```

Check that it worked.

```{r}
str(performance)
```

Or the tidy way.

```{r}
performance %>%
  str()
```

```{r}
lr_fit <- fit_data(high_performer ~ gender + job_level,
                   model = log_spec,
                   data = performance)
```

```{r}
lr_fit %>%
  tidy()
```

Can we pull a report out of that?

```{r, eval = FALSE}
lr_fit %>%
    report()
```
Not yet...

Is that the same as base R?

```{r}
logistic_multiple <- glm(high_performer ~ gender + job_level, family = "binomial", data = performance)
logistic_multiple %>%
  tidy()
```

Yes!

What if we didn't know the engines possibilities for `logistic_reg`?

```{r}
show_engines("logistic_reg")
```

Would that have worked for `linear_reg` too?

```{r}
show_engines("linear_reg")
```


The p-value that we should check is the one on the same row as genderMALE. Since it is lower than 0.05, we can conclude that the result of the test is significant at the 0.05 level, which means there is significant difference between counts of ‘high_performers’ even when job level is taken into account.

We won’t review how to interpret the estimate of coefficients from a logistic regression, but there are two things we do need to know. First, logistic regression coefficients do not have the same meaning as the coefficients produced by a linear regression. Second, we can interpret the sign of the coefficients – that is whether they are positive or negative – the way we generally expect. For example, if one of the variable is significant, as gender is here, we can say that male employees are significantly more likely to be high performers because the estimate on gender is positive. A negative sign would be interpreted as male employees being significantly less likely to be high performers.

### Another aside, the bootstrap

Try to build off of this (https://www.tidymodels.org/learn/statistics/bootstrap/)

This section is from the following [post](https://cran.r-project.org/web/packages/rsample/vignettes/Working_with_rsets.html).

We will perform this portion mostly outside of the `tidymodels` format to contrasts what we did above.

#### Introduction
`rsample` can be used to create objects containing resamples of the original data. This page contains examples of how those objects can be used for data analysis.

For illustration, the `attrition` data is used. From the help file:

These data are from the IBM Watson Analytics Lab. The website describes the data with “Uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. This is a fictional data set created by IBM data scientists.” There are 1470 rows.

The data can be accessed using

```{r}
library(rsample)
library(modeldata) #added after "attrition" disappeared from `rsample`
data("attrition")
names(attrition)
```

```{r}
table(attrition$Attrition)
```

### Model Assessment
Let’s fit a logistic regression model to the data with model terms for the job satisfaction, gender, and monthly income.

If we were fitting the model to the entire data set, we might model attrition using
```{r}
glm(Attrition ~ JobSatisfaction + Gender + MonthlyIncome, data = attrition, family = binomial)
```

```{r}
lr_model <- glm(Attrition ~ JobSatisfaction + Gender + MonthlyIncome, data = attrition, family = binomial)
```


```{r}
summary(lr_model)
```

What kind of model was that? Why?

For convenience, we’ll create a formula object that will be used later:

```{r}
mod_form <- as.formula(Attrition ~ JobSatisfaction + Gender + MonthlyIncome)
```

Could we also do this like we did for our Linear Regression using `workflows()` from `tidymodels`?

Yes!

Now, write a pipe that creates a model that uses `glm()` to fit a logistic regression. Save it as `logit_spec` and look at the object. What does it return?

Hint: you'll need (https://tidymodels.github.io/parsnip/articles/articles/Models.html)

```{r}
logit_spec <- 
  logistic_reg(mode = "classification") %>% # Pick logistic regression and set mode to "classification"
  set_engine(engine = "glm") # set engine

logit_spec
```

`fit_data`-train a model by fitting a model. Returns a `parsnip` model fit.
```{r}
fit_data(Attrition ~ JobSatisfaction + Gender + MonthlyIncome, model = logit_spec, data = attrition)
```

```{r}
logit_fit <- fit_data(Attrition ~ JobSatisfaction + Gender + MonthlyIncome,
                   model = logit_spec,
                   data = attrition)

logit_fit
```

Can we get a `summary` out of `logit_fit`?

```{r}
summary(logit_fit)
```

Hmmm, that is not what we were looking for. What if we wrap it in `tidy`?
```{r, eval = FALSE}
tidy(logit_fit)
```

There we go. 

Now lets arrange that from smallest to largest statistic.
```{r}
tidy(logit_fit) %>%
  arrange(desc(abs(statistic)))
```


Ok, so, are `lr_model` and `logit_model` the same?

```{r}
#Make predictions
probabilities <- lr_model %>%
  predict(attrition, type = "response")
```


```{r}
logit_fit <- fit_data(Attrition ~ JobSatisfaction + Gender + MonthlyIncome,
                   model = logit_spec,
                   data = attrition)

attrit_pred <- logit_fit %>%
  predict(new_data = attrition) %>%
  mutate(truth = attrition$Attrition)

attrit_pred
```

Hmmm, that is giving us a decision, Yes or No. Is there a way we can actually get at the true probability (e.g. a number between 0 and 1)?


```{r}
attrit_pred <- logit_fit %>%
  predict(new_data = attrition, type = "prob") %>%
  mutate(truth = attrition$Attrition)

attrit_pred
```

And what were the probabilities from the `lr_model`?

```{r}
probabilities %>% tibble()
```

Those look the same to me out to 8 decimals so I will call it the same. Not sure what is going on with the dummy variable coding in the `lr_model` vs the `logit_spec`.

Anyway, onward we go.

To evaluate this model, we will use 10 repeats of 10-fold cross-validation and use the 100 holdout samples to evaluate the overall accuracy of the model with our old friend `vfold_cv`.

First, let’s make the splits of the data:

```{r}
library(rsample)
set.seed(4622)
rs_obj <- vfold_cv(attrition, v = 10, repeats = 10) #remember, we previously called this `cv_folds` for our lr model
rs_obj
```

Wait a minute...what just happened? Did we just do a 10-fold cross-validation 10 times?

Now let’s write a function that will, for each resample:

1. obtain the analysis data set (i.e. the 90% used for modeling)
2. fit a logistic regression model
3. predict the assessment data (the other 10% not used for the model) using the broom package
4. determine if each sample was predicted correctly.

Here is our function:

```{r}
## splits will be the `rsplit` object with the 90/10 partition
holdout_results <- function(splits, ...) {
  # Fit the model to the 90%
  mod <- glm(..., data = analysis(splits), family = binomial)
  # Save the 10%
  holdout <- assessment(splits)
  # `augment` will save the predictions with the holdout data set
  res <- broom::augment(mod, newdata = holdout)
  # Class predictions on the assessment set from class probs
  lvls <- levels(holdout$Attrition)
  predictions <- factor(ifelse(res$.fitted > 0, lvls[2], lvls[1]),
                        levels = lvls)
  # Calculate whether the prediction was correct
  res$correct <- predictions == holdout$Attrition
  # Return the assessment data set with the additional columns
  res
}
```

For example:

```{r}
example <- holdout_results(rs_obj$splits[[1]],  mod_form)
dim(example)
#> [1] 147  35
dim(assessment(rs_obj$splits[[1]]))
#> [1] 147  31
## newly added columns:
example[1:10, setdiff(names(example), names(attrition))]
```

For this model, the `.fitted` value is the linear predictor in log-odds units.

To compute this data set for each of the 100 resamples, we’ll use the `map` function from the `purrr` package:

```{r}
library(purrr)
rs_obj$results <- map(rs_obj$splits,
                      holdout_results,
                      mod_form)
rs_obj
```

Now we can compute the accuracy values for all of the assessment data sets:

```{r}
rs_obj$accuracy <- map_dbl(rs_obj$results, function(x) mean(x$correct))
summary(rs_obj$accuracy)
```

Keep in mind that the baseline accuracy to beat is the rate of non-attrition, which is 0.839. Not a great model so far.

### Using the Bootstrap to Make Comparisons

Traditionally, the bootstrap has been primarily used to empirically determine the sampling distribution of a test statistic. Given a set of samples with replacement, a statistic can be calculated on each analysis set and the results can be used to make inferences (such as confidence intervals).

For example, are there differences in the median monthly income between genders?

```{r}
ggplot(attrition, aes(x = Gender, y = MonthlyIncome)) + 
  geom_boxplot() + 
  scale_y_log10()
```

If we wanted to compare the genders, we could conduct a t-test or rank-based test. Instead, let’s use the bootstrap to see if there is a difference in the median incomes for the two groups. We need a simple function to compute this statistic on the resample:

```{r}
median_diff <- function(splits) {
  x <- analysis(splits)
  median(x$MonthlyIncome[x$Gender == "Female"]) - 
      median(x$MonthlyIncome[x$Gender == "Male"])     
}
```

This will take the median difference of the sample of Female vs Male montly income by split.


```{r}
attrition %>%
    select(Gender,
           MonthlyIncome) %>%
    slice_head(n = 5)
```


Now we would create a large number of bootstrap samples (say 2000+). For illustration, we’ll only do 500 in this document.

```{r}
set.seed(353)
bt_resamples <- bootstraps(attrition, times = 500)
```

What does that look like?

```{r}
bt_resamples$splits[[1]]
```

What does that mean?

The output you provided represents a summary of one of the bootstrap samples obtained from the `bt_resamples` object. Let's break down the information:

- `bt_resamples$splits`: This indicates that the `bt_resamples` object contains a list of bootstrap samples. Each bootstrap sample is stored as an element within this list.

- `[[1]]`: The `[[1]]` part refers to the first bootstrap sample within the list. In this case, the summary is specific to the first bootstrap sample.

- `<Analysis/Assess/Total>`: This line provides a breakdown of the number of observations in the bootstrap sample. It represents the analysis set, assessment set, and total number of observations in the sample.

  - `1470`: The "Analysis" part indicates the number of observations included in the analysis set. In this case, the bootstrap sample consists of 1470 observations.

  - `558`: The "Assess" part indicates the number of observations included in the assessment set. The assessment set is typically used for evaluating model performance or validation purposes. In this case, the bootstrap sample has 558 observations in the assessment set.

  - `1470`: The "Total" part indicates the total number of observations in the bootstrap sample. In this case, the bootstrap sample has a total of 1470 observations.

Overall, this output tells us that the first bootstrap sample extracted from `bt_resamples` consists of 1470 observations, with 545 observations allocated for assessment purposes.

Ok, so then what do we do?

This function is then computed across each resample:

```{r}
bt_resamples$wage_diff <- map_dbl(bt_resamples$splits, median_diff)
```

```{r}
bt_resamples$wage_diff
```



The bootstrap distribution of this statistic has a slightly bimodal and skewed distribution:

```{r}
ggplot(bt_resamples, aes(x = wage_diff)) + 
  geom_line(stat = "density", adjust = 1.25) + 
  xlab("Difference in Median Monthly Income (Female - Male)")
```

The variation is considerable in this statistic. One method of computing a confidence interval is to take the percentiles of the bootstrap distribution. A 95% confidence interval for the difference in the means would be:

```{r}
quantile(bt_resamples$wage_diff, 
         probs = c(0.025, 0.500, 0.975))
```

*On average*, there is no evidence for a difference in the genders. Our 95% CI contains zero.

### Bootstrap Estimates of Model Coefficients
Unless there is already a column in the resample object that contains the fitted model, a function can be used to fit the model and save all of the model coefficients. The `broom` [package](https://cran.r-project.org/web/packages/broom/index.html) has a `tidy` function that will save the coefficients in a data frame. Instead of returning a data frame with a row for each model term, we will save a data frame with a single row and columns for each model term. As before, `purrr::map` can be used to estimate and save these values for each split.

```{r}
glm_coefs <- function(splits, ...) {
  ## use `analysis` or `as.data.frame` to get the analysis data
  mod <- glm(..., data = analysis(splits), family = binomial)
  as.data.frame(t(coef(mod)))
}
bt_resamples$betas <- map(.x = bt_resamples$splits, 
                          .f = glm_coefs, 
                          mod_form)
bt_resamples
```
Ahh, so now we have 500 regression models?

Yes, here is what the first one looks like:


```{r}
bt_resamples$betas[[1]]
```

Here is what the second one looks like:

```{r}
bt_resamples$betas[[2]]
```
And so on all the way to 500.


### Keeping Tidy
As previously mentioned, the `broom` package contains a class called `tidy` that created representations of objects that can be easily used for analysis, plotting, etc. `rsample` contains `tidy` methods for `rset` and `rsplit` objects. For example:

```{r}
first_resample <- bt_resamples$splits[[1]]
class(first_resample)
```


```{r}
tidy(first_resample)
```

and

```{r}
class(bt_resamples)
```

```{r}
tidy(bt_resamples)
```

More on why bootstrapping works at [Cross Validated](https://stats.stackexchange.com/questions/26088/explaining-to-laypeople-why-bootstrapping-works).

Also see 21.3 Bootstrapping Regression Models [here](https://www.sagepub.com/sites/default/files/upm-binaries/21122_Chapter_21.pdf) and this [paper](http://statweb.stanford.edu/~tibs/sta305files/FoxOnBootingRegInR.pdf) by John Fox using the same data with R.

We'll now do another example using bootstrapping from [here](https://afit-r.github.io/bootstrapping).

## Bootstrapping for Parameter Estimates

Resampling methods are an indispensable tool in modern statistics. They involve repeatedly drawing samples from a training set and recomputing an item of interest on each sample. Bootstrapping is one such resampling method that repeatedly draws independent samples from our data set and provides a direct computational way of assessing uncertainty. This tutorial covers the basics of bootstrapping to estimate the accuracy of a single statistic. Note that resampling (to include bootstrapping) to improve the bias-variance tradeoff in predictive modeling is [discussed elsewhere](https://afit-r.github.io/resampling_methods).


### tl;dr
First, I cover the packages and data used to reproduce results displayed in this tutorial. I then discuss how boostrapping works followed by illustrating how to implement the method in R.

* Prerequisites: What you need.
* Why Bootstrap?: A short discussion of how boostrapping works.
* Creating boostrap samples: How do you create bootstrap samples in R.
* Applying functions: How to iterate over each sample to compute a parameter estimate.
* Where to learn more: Resources to help you learn more.

### Prerequisites
This tutorial primarily uses the `rsample` package to create bootstrap samples but also uses the `purrr` and `ggplot2` packages (both contained in `tidyverse`). To illustrate, our example `attrition` data comes from the `rsample` package.

```{r}
library(tidyverse)
library(rsample)

as_tibble(attrition)
```

### Why Bootstrap?
Direct standard error formulas as discussed in the [univariate statistical inference tutorial](https://afit-r.github.io/univariate_inference#mean) exist for various statistics and help to compute confidence intervals. However, prior to the computer age, when estimating certain parameters of the distribution (i.e. percentile points, proportions, odds ratio, and correlation coefficients), complex and laborious Taylor series were required to compute errors of an estimate. The bootstrap was developed as an alternative, computer-intensive approach to derive estimates of standard errors and confidence intervals for _any_ statistics.

Bootstrapping involves repeatedly drawing independent samples from our data set (Z) to create bootstrap data sets (
Z∗1, Z∗2,…,Z∗B). This sample is performed with _replacement_, which means that the same observation can be sampled more than once and each bootstrap sample (Z∗i) will have the same number of observations as the original data set (see Figure 5.11 in ISLR).

The basic idea of bootstrapping is that inference about a population from sample data, can be modeled by resampling the sample data and performing inference about a sample from resampled data (resampled → sample → population).

The bootstrap process begins with a statistic that we are interested in (^α∗). Some large number (B) of bootstrap samples are independently drawn. Each bootstrap sample is used to compute this statistic (^α∗1,^α∗2,…,^α∗B). We can then use all the bootstrapped data sets to compute the standard error of this desired statistic. (See Equation 5.8 in ISLR) or if you want to learn more about writing equations in RMarkdown go [here](https://www.statpower.net/Content/310/R%20Stuff/SampleMarkdown.html).

Thus, ˆSEB serves as an estimate of the standard error of ^α estimated from the original data set.

#### Wait, why would I want to do this over a traditional t-test?
The advantage of using bootstrapping in this case is that it does not rely on assumptions about the underlying distribution of the data or the test statistic. It empirically estimates the sampling distribution by resampling the observed data multiple times. This makes it more robust to violations of assumptions and applicable to a wider range of scenarios.
In contrast, a t-test for comparing medians would require assumptions such as normality and equal variances, which may not hold in this data set. Bootstrapping provides a non-parametric alternative that can still quantify the uncertainty in the difference between the medians, without making strong distributional assumptions.

### Creating Bootstrap Samples
The first objective in bootstrapping is to create your B bootstrap samples. We can use `rsample::bootstrap` to create an object that contains _B_ resamples of the original data.

```{r}
# reproducibility
set.seed(124)

# create 10 bootstap samples
(bt_samples <- bootstraps(attrition, times = 10))
```

The bootstrap samples are stored in data-frame-like `tibble` object where each bootstrap is nested in the `splits` column. We can access each bootstrap sample just as you would access parts of a list. Here, we access the first bootstrap sample stored in `bt_samples$splits[[1]]`. The `<1470/530/1470>` indicates that 1470 observations are in the bootstrap sample, 530 observations were not sampled in this bootstrap sample (meaning 1470−530=940 observations were sampled 1 or more times to make the 1470 observations in the bootstrap sample), and the last 1470 indicates the total number of observations in the original data set. Remember, when performing bootstrap sampling, each bootstrap sample will always contain the same number of observations as the original data set. To extract the first bootstrap sample we can use `rsample::analysis`.

```{r}
# observation split
bt_samples$splits[[1]]
## <1470/523/1470>

# extract the first bootstrap sample
analysis(bt_samples$splits[[1]]) %>% as_tibble()
```

So we have successfully created bootstrap samples and we know how to access each sample. Now its time to apply a function iteratively over each bootstrap sample to compute our desired statistic.

### Applying Functions Across Bootstrap Samples
Assume we wanted to understand the difference in income for employees that churn versus those that do not. As discussed in the [multivariate statistical inference tutorial](https://afit-r.github.io/multivariate_inference) we can use the two-sample _t_-test to compare the difference in mean income. But what if we want to compare the difference in median income? Or the difference in the 95th percentile income?

There is no simple test statistic to perform these comparisons; however, we can use bootstrapping to get a better understanding of these estimates and their standard errors. First, we’ll create a function that computes the difference in median monthly incomes between employees that churn versus those that do not.

```{r}
statistic <- function(splits) {
  x <- analysis(splits)
  median_yes <- median(x$MonthlyIncome[x$Attrition == "Yes"]) 
  median_no <- median(x$MonthlyIncome[x$Attrition == "No"]) 
  median_yes - median_no
}
```

The above function takes an individual bootstrap sample (i.e. split) and computes the difference in median income. Now let’s create our bootstrap samples. There is no concrete number of bootstrap samples you should use but a good rule of thumb is to use at least 500 when computing a measure of central tendency and at least 1000 when computing an extreme value such as the 95th percentile. With today’s modern computing power, my default is to use a minimum of 2000.

```{r}
set.seed(123)
bt_samples <- bootstraps(attrition, times = 2000)
```

We can now iterate over each bootstrap sample and apply our statistic. Here, we use `map_dbl`, which comes from the `purrr` package and is part of the `tidyverse`, to iterate over each sample and compute the difference in median incomes. The results show a slightly bimodal and skewed distribution.

```{r}
# iterate over each bootstrap sample and compute statistic
bt_samples$wage_diff <- map_dbl(bt_samples$splits, statistic)

# plot distribution
ggplot(bt_samples, aes(x = wage_diff)) + 
  geom_line(stat = "density", adjust = 1.25) + 
  xlab("Difference in Median Monthly Income (Attrition minus Non-Attrition)")
```

Hmmm, that looks different from the distribution from the website. What is going on? We set the same seed...or did we?

The reason for the inconsistency is that in R 3.6.0, [the default kind of under-the-hood random-number generator was changed](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Random.html). Now, in order to get the results from `set.seed()` to match, you have to first call the function `RNGkind(sample.kind = "Rounding")`.

```{r}
#From: https://stackoverflow.com/questions/47199415/is-set-seed-consistent-over-different-versions-of-r-and-ubuntu
RNGkind(sample.kind = "Rounding")
set.seed(123)
bt_samples <- bootstraps(attrition, times = 2000)
```

```{r}
# iterate over each bootstrap sample and compute statistic
bt_samples$wage_diff <- map_dbl(bt_samples$splits, statistic)

# plot distribution
ggplot(bt_samples, aes(x = wage_diff)) + 
  geom_line(stat = "density", adjust = 1.25) + 
  xlab("Difference in Median Monthly Income (Attrition minus Non-Attrition)")
```
Ok, now that looks the same.

We see that across all bootstrap samples, this statistic ranges from about -2500 to -1000. We can compute the confidence interval by taking the percentiles of the bootstrap distribution. We see that the median difference is -$1,949 with a 95% confidence interval between -$2,355 and -$1,409.

```{r}
quantile(bt_samples$wage_diff, probs = c(.05, .5, .95))
```

### Resource to Learn More
Bootstrapping has many useful applications ranging from simple parameter estimates to being incorporated into modeling approaches (i.e. Random Forests). You can learn more about boostrapping, and its application in R, with the following resources:

* [Bootstrap Methods and Their Application](https://www.amazon.com/Bootstrap-Application-Statistical-Probabilistic-Mathematics/dp/0521574714)
* [Computer Age Statistical Inference](https://www.amazon.com/Computer-Age-Statistical-Inference-Mathematical/dp/1107149894/ref=sr_1_1?ie=UTF8&qid=1517083963&sr=8-1&keywords=computer+age+statistical+inference)
* [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)
* [An Introduction to Statistical Learning](https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)
* [Introduction to Statistics Through Resampling Methods and R](https://www.amazon.com/Introduction-Statistics-Through-Resampling-Methods/dp/1118428218)

## Back to the main post

Ok, now back to the main post.

### Improving Employee Safety with Data

In many industries, workplace safety is a critical consideration. According to our dataset, we can see that workplace accidents have increased past few years at the production sites, where accidents are more likely to happen. First we want to find if that is true, and if it is to look into what might be the cause of increase in workplace accidents.

To tackle this analysis, we will first need to join the ‘accidents’ dataset with ‘hr’ dataset. Since the datasets spans both 2016 and 2017, when we join on ’employee id’ left_join won’t know which row in ‘hr’ to join the ‘accident’ data to. However, if we join on both employee id and year, the combination is enough to identify rows uniquely. We can join on multiple columns or keys, by passing them to the ‘by’ argument of the left_join() function.

```{r importingh}
# HR Data

#From: https://stackoverflow.com/questions/14441729/read-a-csv-from-github-into-r
library(readr)
hr_data <- read_csv("https://raw.githubusercontent.com/Dragana236/Data_analysis/master/hr_2.csv")

head(hr_data)

# Accident Data

accident_data <- read_csv("https://raw.githubusercontent.com/Dragana236/Data_analysis/master/accident_data.csv")

head(accident_data)

# Join the data

hr_joined <- left_join(hr_data, accident_data, by = c("employee_id", "year")) 
head(hr_joined)
```

Since we are using left join, all employee id’s from left table will be kept and if there is no such id in the right table (employee didn’t have an accident), it will be filled with NAs.


We could’ve filtered only employees that had accidents with is.na() function and ! operator. However, we will keep both, employees that didn’t have and these that had accident. For this purpose we can again use ifelse() function, marking with 1 those that had an accident and with 0 those that didn’t have.


```{r mutate}
hr_joined <- hr_joined %>%
  mutate(had_accident = ifelse(is.na(accident_type), 0, 1))
```

The dataset is ready for analysis now.

Notice that we could’ve also written last two codes together with pipe operator.

```{r}
# hr_joined <- left_join(hr_data, accident_data,
#                        by = c('employee_id', 'year')) %>%
#     mutate(had_accident = ifelse(is.na(accident_type), 0,1))
```


It’s time to answer on the question if accident rate increased from 2016 to 2017. Now that we have the ‘had_accident’ variable, we can calculate the accident rate by taking the mean of this variable.

```{r}
hr_joined %>% 
  group_by(year) %>%  
  summarise(accident_rate = round(mean(had_accident),2)) 
```

We can see that there is an increase in 2017 year. To test whether the accident rate was higher in either year we will use chi-squared test. We can see that there is significant increase in accident rates from 2016 to 2017 year at 0.05 level.

```{r tesm}
chisq.test(hr_joined$year, hr_joined$had_accident) %>%
    report()
```

Now, let’s see which site has the highest accident rate. Grouped, summarised and arranged data is shown below.

```{r}
hr_joined %>% 
  group_by(location) %>%  
  summarise(accident_rate = round(mean(had_accident),2)) %>% 
  arrange(desc(accident_rate))
```

The location with the highest accident rate is a fine place to start, but to answer our main question, it will be more productive to focus on the location where the accident rate increased most from last year. To answer this question we pass multiple arguments to `group_by()`, so that summarise() performs its calculations on each location in each year. What we get is following dataset.

```{r te}
accident_rates <- hr_joined %>% 
  group_by(location, year) %>% 
  summarise(accident_rate = round(mean(had_accident),2)) %>%
    arrange(desc(accident_rate)) %>%
  ungroup()

accident_rates
```

We created faceted bar plots with ‘factor(year)’ on the x-axis and accident rate on the y-axis for each location. We can see that the biggest increase happened in Southfield.

```{r}
accident_rates %>% 
  ggplot(aes(factor(year), accident_rate)) +
  geom_col() +
  facet_wrap(~location)
```

Because Southfield was the location where the accident increased the most from 2016 to 2017, we will investigate what else changed there. Overworked employees are more likely to make mistakes that lead to accidents, so we can start by comparing average overtime hours worked in each year.

After filtering rows for ‘Southfield’, we grouped and calculated the average overtimes for each year.

```{r tas}
southfield <- hr_joined %>% 
  filter(location == "Southfield")
southfield %>%
  group_by(year) %>% 
  summarise(average_overtime_hours = round(mean(overtime_hours),2)) %>%
  ungroup()
```

To confirm that average of overtime hours was higher in either year, we will use t-test to test it. From following we can say that difference in average overtime hours is not significant at 0.05 level since the p-value is higher than 0.05.

```{r tb}
t.test(overtime_hours ~ year, data = southfield) %>%
    report()
```

Since overtime hours didn’t have a significant change between the years, we should see what other variables we can check. We can try with engagement scores, and specifically with the number of disengaged employees at the location. We don’t have the survey data ready yet, so we’ll need to load it in and join it to the data we’ve been working with. After joining we also added new column in which disengaged employees were all employees with engagement score lower or equal to 2.

```{r ct}
# Survey Data

#From: https://stackoverflow.com/questions/14441729/read-a-csv-from-github-into-r
library(readr)
survey_data <- read_csv("https://raw.githubusercontent.com/Dragana236/Data_analysis/master/survey_2.csv")

head(survey_data)
```


```{r}
safety <- left_join(hr_joined, survey_data, by = c("year", "employee_id")) %>% 
  mutate(disengaged = ifelse(engagement <= 2, 1, 0), 
         year = factor(year))
safety_southfield <- safety %>% 
  filter(location == "Southfield")
```


Now we can see if employee engagement could be an omitted variable that would explain the difference in the annual accident rates.

With above created dataset we can test our idea that employee engagement, or specifically, employee disengagement, changed at same time when the accident rate changed. First we used a bar chart to see if number of disengaged employees increased over two years in Southfield. We can observe increase.

```{r t}
safety_southfield %>% 
  ggplot(aes(x = year, fill = factor(disengaged))) +
  geom_bar(position = "fill")
```
Let’s now use a statistical test to check if the difference we saw is significant. Since the p-value is lower than 0.05 we can say that difference in rate of disengaged over two year is significant.

```{r}
chisq.test(safety_southfield$year, safety_southfield$disengaged) %>%
    report()
```

We’ve found a difference in disengagement between the two years at Southfield, and no difference in overtime hours worked. Now we’ll check other locations too, to make sure those locations didn’t also have a big change in disengagement. If the disengagement rate increased at the other locations where the accident rate did not change much, we’ll have to doubt whether disengagement and accident rates are connected.

To test the other locations all at once, we created a new dataset where Southfield is filtered out.

```{r t9}
other_locs <- safety %>% 
  filter(location != "Southfield")
```

Then, we rerun the same statistical tests but this time on this new dataset with other locations. First, we tested whether one year had significantly more overtime hours worked. Since the p-value is higher than 0.05 we can state that difference in overtime hours worked is not statistically significant.

```{r}
t.test(overtime_hours ~ year, data = other_locs) %>%
    report()
```

Next we used chi-squared test to test whether one year had significantly more disengaged employees. Same as before, difference in rate of disengaged is not statistically significant.

```{r}
chisq.test(other_locs$year, other_locs$disengaged) %>%
    report()
```

Finally, let’s do one more test on the connection between disengagement and accident rates. We will use multiple regression to answer the question why accident rate increased. Phrased another way, we want to see if there is a variable in our dataset that, when added into a multiple regression, can explain the difference in accident rates between the two years. We confirmed with statistical test that rate of disengagement can be the cause for increase of accident rates. We now want to confirm it with multiple logistic regression. Therefore, we test whether one year had a significantly higher accident rate, even when taking disengagement into account.

```{r}
log_regression <- glm(had_accident ~ year + disengaged, family = "binomial", data = safety_southfield)
tidy(log_regression)
```

```{r}
log_regression %>%
    report()
```


Our analysis uncovered a statistically significant connection between employee’s disengagement and accident rates at Southfield. In fact, employee’s disengagement can explain the difference in accident rates between years. However, even though we used regression, we don’t know exactly how to interpret it without more analysis. Correlation – even from a regression – does not imply causation, nor does it tell us which direction the potential causation might go.

# Just the code

```{r ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE}

```